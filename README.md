ASL Sign Language Recognition – DAEN 429 Project
This repository contains the full code and experiment setup for a two-stage sign language recognition project:

Phase 1 – Static ASL Alphabet Classification (Images)
Ablation experiments: T-A, T-B, T-C, S-A
Best Model: T-C
Uses ResNet-18 with transfer learning

Phase 2 – WLASL100 Word-Level Recognition (Videos)
Feature extractor: Best Phase 1 model
Temporal model: LSTM
Reduced dataset for computational feasibility


Project Structure
- DAEN429_ASL_Project.ipynb    # Full pipeline: Phase 1 + Phase 2
    - Data Loading
    - Train/Validation Splitting
    - Ablation Experiments
    - Model Training
    - Evaluation on official + custom test sets
    - WLASL100 video processing
    - Phase 2 LSTM training


Datasets (NOT Included):
- ASL Alphabet Dataset
- WLASL100 Dataset

How to run the notebook:
1. Upload the notebook to Google Colab
2. Mount Google Drive:
   - from google.colab import drive
      drive.mount('/content/drive')
3. Adjust dataset paths in configuration.py, for example:
   - ASL_TRAIN_SET = "/content/drive/MyDrive/DAEN429Project/asl_alphabet_train"
     ASL_TEST_SET = "/content/drive/MyDrive/DAEN429Project/asl_alphabet_test"
     WLASL100_SET = "/content/drive/MyDrive/DAEN429Project/WLASL100"
4. Run all cells

Note: The notebook uses reduced training sizes to allow training within Colab's compute limits.

Reproducibility Notes
- All random seeds were fixed to ensure consistency.
- Models are trained on Colab T4 GPUs
- Experiments require PyTorch, Torchvision, scikit-learn, OpenCV, numpy, and matplotlib.
- Checkpoints (.pt files) are not included, but can be regenerated by running the notebook.
