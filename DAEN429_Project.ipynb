{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzVg3Oow-tUF",
        "outputId": "41787d41-b4e0-431f-ea2f-94f0579bbb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/DAEN429Project\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2mcRorVPkir",
        "outputId": "627d2ea6-fcc2-406e-95c7-ddb678d9514b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asl_alphabet_test   checkpoints      figures\t      WLASL_100\n",
            "asl_alphabet_train  DAEN429_Project  Untitled0.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seed\n",
        "def global_seed(seed = 429):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "global_seed()\n",
        "\n",
        "class configuration:\n",
        "  DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  ASL_TRAIN_SET = \"/content/drive/MyDrive/DAEN429Project/asl_alphabet_train\"\n",
        "  ASL_TEST_SET = \"/content/drive/MyDrive/DAEN429Project/asl_alphabet_test\"\n",
        "\n",
        "  WLASL100_SET = \"/content/drive/MyDrive/DAEN429Project/WLASL_100\"\n",
        "\n",
        "  FRAME_NUM = 16\n",
        "\n",
        "  # Model Settings\n",
        "  ASL_CLASSES_NUM = 29\n",
        "  BATCH_SIZE = 128\n",
        "  NUM_WORKERS = 0\n",
        "  LR = 1e-3\n",
        "  WEIGHT_DECAY = 1e-4\n",
        "  TA_EPOCHS = 2\n",
        "  TB_EPOCHS = 2\n",
        "  TC_EPOCHS = 2\n",
        "  SA_EPOCHS = 3\n",
        "  P2_EPOCHS = 5\n",
        "  P2_LR = 1e-4\n",
        "\n",
        "  CHECKPOINT_DIR = \"/content/drive/MyDrive/DAEN429Project/checkpoints\"\n",
        "  FIGURE_DIR = \"/content/drive/MyDrive/DAEN429Project/figures\"\n",
        "\n",
        "# Create directories for saving results\n",
        "os.makedirs(configuration.CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(configuration.FIGURE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Setup complete. Using device: {configuration.DEVICE}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "otRZ99J0Ag4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9916fe-6649-4c86-b518-3d94a73ead22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness = 0.2, contrast = 0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Building train/val split (seed = 429)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def build_asl_train_val(seed=429, reduction_ratio=0.005):\n",
        "\n",
        "    print(f\"\\n=== Building ASL Train/Val Split (Using {int(reduction_ratio*100)}% of dataset) ===\")\n",
        "\n",
        "    # Load full dataset once\n",
        "    full_train = datasets.ImageFolder(configuration.ASL_TRAIN_SET, transform=train_transform)\n",
        "    class_names = full_train.classes\n",
        "\n",
        "    indices = np.arange(len(full_train))\n",
        "    labels = np.array(full_train.targets)\n",
        "\n",
        "    # Reduction\n",
        "    reduced_idx, _ = train_test_split(\n",
        "        indices,\n",
        "        train_size=reduction_ratio,\n",
        "        stratify=labels,\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    print(f\"Original dataset size: {len(full_train)}\")\n",
        "    print(f\"Reduced dataset size:  {len(reduced_idx)}\")\n",
        "\n",
        "    # Build reduced dataset for training transforms\n",
        "    reduced_dataset_train = Subset(full_train, reduced_idx)\n",
        "\n",
        "    full_train_eval = datasets.ImageFolder(configuration.ASL_TRAIN_SET, transform=eval_transform)\n",
        "    reduced_dataset_eval = Subset(full_train_eval, reduced_idx)\n",
        "\n",
        "    # Extract labels of reduced subset for second split\n",
        "    reduced_labels = labels[reduced_idx]\n",
        "\n",
        "    # Train/Val Split (80/20)\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        np.arange(len(reduced_idx)),\n",
        "        test_size=0.2,\n",
        "        stratify=reduced_labels,\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    train_dataset = Subset(reduced_dataset_train, train_idx)\n",
        "    val_dataset = Subset(reduced_dataset_eval, val_idx)\n",
        "\n",
        "    print(f\"Final Train size: {len(train_dataset)}\")\n",
        "    print(f\"Final Val size:   {len(val_dataset)}\")\n",
        "    print(f\"Classes: {class_names}\")\n",
        "\n",
        "    return train_dataset, val_dataset, class_names\n",
        "\n",
        "\n",
        "\n",
        "# Flat Test Folder Dataset\n",
        "class ASLTestSet(Dataset):\n",
        "  def __init__(self, root, transform, class_to_idx):\n",
        "    self.root = root\n",
        "    self.transform = transform\n",
        "    self.samples = []\n",
        "    self.class_to_idx = class_to_idx\n",
        "\n",
        "    for fname in os.listdir(root):\n",
        "      if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        label_name = fname.split(\"_\")[0]\n",
        "        label_name = label_name.strip()\n",
        "\n",
        "        key1 = label_name.lower()\n",
        "        key2 = label_name.upper()\n",
        "\n",
        "        if key1 in class_to_idx:\n",
        "          label = class_to_idx[key1]\n",
        "        elif key2 in class_to_idx:\n",
        "          label = class_to_idx[key2]\n",
        "        else:\n",
        "          raise ValueError(\"Unknown test label.\")\n",
        "\n",
        "        path = os.path.join(root, fname)\n",
        "        self.samples.append((path, label))\n",
        "\n",
        "    print(f\"Loaded {len(self.samples)} test images from {root}\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    path, label = self.samples[idx]\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    img = self.transform(img)\n",
        "    return img, label\n",
        "\n",
        "# Building Model Resnet-18\n",
        "def build_resnet18_model(num_classes = configuration.ASL_CLASSES_NUM, pretrained = True):\n",
        "  model = models.resnet18(pretrained = pretrained)\n",
        "  num_ftrs = model.fc.in_features\n",
        "  model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "  return model\n",
        "\n",
        "  # Freezing/Unfreezing Policies, Ablations\n",
        "def freeze_policy(model):\n",
        "  for name, p in model.named_parameters():\n",
        "    if not name.startswith(\"fc.\"):\n",
        "      p.requires_grad = False\n",
        "\n",
        "\n",
        "def freeze_except_layer4(model):\n",
        "  for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "  for name, p in model.named_parameters():\n",
        "    if name.startswith(\"layer4\") or name.startswith(\"fc.\"):\n",
        "      p.requires_grad = True\n",
        "\n",
        "def unfreeze_layer3_and_4(model):\n",
        "  for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "  for name, p in model.named_parameters():\n",
        "    if name.startswith(\"layer3\") or name.startswith(\"layer4\") or name.startswith(\"fc.\"):\n",
        "      p.requires_grad = True\n",
        "\n",
        "# Training Utilities\n",
        "def compute_metrics(y_true, y_pred):\n",
        "  return {\n",
        "      \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "      \"f1\": f1_score(y_true, y_pred, average = \"macro\")\n",
        "  }\n",
        "\n",
        "def run_epoch(model, loader, criterion, optimizer = None):\n",
        "  train_mode = optimizer is not None\n",
        "\n",
        "  model.train(train_mode)\n",
        "  total_loss = 0\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  for x, y in loader:\n",
        "    x = x.to(configuration.DEVICE)\n",
        "    y = y.to(configuration.DEVICE)\n",
        "\n",
        "    if train_mode:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    logits = model(x)\n",
        "    loss = criterion(logits, y)\n",
        "\n",
        "    if train_mode:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    total_loss += loss.item() * x.size(0)\n",
        "\n",
        "    preds = logits.argmax(dim = 1)\n",
        "    y_true.extend(y.cpu().numpy())\n",
        "    y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "  avg_loss = total_loss / len(loader.dataset)\n",
        "  metrics = compute_metrics(y_true, y_pred)\n",
        "\n",
        "  return avg_loss, metrics, y_true, y_pred\n",
        "\n",
        "\n",
        "# Training Loop / Model\n",
        "def train_model(model, train_loader, val_loader, num_epochs, exp_name):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr = configuration.LR,\n",
        "        weight_decay = configuration.WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    optimal_f1 = -1\n",
        "    best_state = None\n",
        "\n",
        "    hist = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"train_f1\": [],\n",
        "        \"val_f1\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_metrics, _, _ = run_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_metrics, _, _ = run_epoch(model, val_loader, criterion)\n",
        "\n",
        "        hist[\"train_loss\"].append(train_loss)\n",
        "        hist[\"val_loss\"].append(val_loss)\n",
        "        hist[\"train_f1\"].append(train_metrics[\"f1\"])\n",
        "        hist[\"val_f1\"].append(val_metrics[\"f1\"])\n",
        "\n",
        "        print(f\"[{exp_name}] Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train F1={train_metrics['f1']:.4f} | Val F1={val_metrics['f1']:.4f}\")\n",
        "\n",
        "        if val_metrics[\"f1\"] > optimal_f1:\n",
        "            optimal_f1 = val_metrics[\"f1\"]\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "        checkpt_path = f\"{configuration.CHECKPOINT_DIR}/{exp_name}_best.pt\"\n",
        "        torch.save(best_state, checkpt_path)\n",
        "        print(\"Model Saved.\")\n",
        "\n",
        "    # load best checkpoint AFTER all epochs\n",
        "    model.load_state_dict(best_state)\n",
        "    return model, hist\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vLw_utSzEO7T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Ablations, and Eval\n",
        "\n",
        "def plot_history(hist, title):\n",
        "  fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n",
        "  ax[0].plot(hist[\"train_loss\"], label = \"Train Loss\")\n",
        "  ax[0].plot(hist[\"val_loss\"], label = \"Val Loss\")\n",
        "  ax[0].set_title(f\"{title} Loss\")\n",
        "  ax[0].legend()\n",
        "\n",
        "  ax[1].plot(hist[\"train_f1\"], label = \"Train F1\")\n",
        "  ax[1].plot(hist[\"val_f1\"], label = \"Val F1\")\n",
        "  ax[1].set_title(f\"{title} F1\")\n",
        "  ax[1].legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "train_dataset, val_dataset, class_names = build_asl_train_val()\n",
        "train_loader = DataLoader(train_dataset, batch_size = configuration.BATCH_SIZE, shuffle = True, num_workers = configuration.NUM_WORKERS)\n",
        "val_loader = DataLoader(val_dataset, batch_size = configuration.BATCH_SIZE, shuffle = False, num_workers = configuration.NUM_WORKERS)\n",
        "\n",
        "results = {}\n",
        "histories = {}\n",
        "\n",
        "# Running T-A: Head Only Fine Tuning\n",
        "print(\"\\n===== Running T-A (Head Only) =====\")\n",
        "model_ta = build_resnet18_model(pretrained = True).to(configuration.DEVICE)\n",
        "freeze_policy(model_ta)\n",
        "\n",
        "model_ta, hist_ta = train_model(model_ta, train_loader, val_loader, configuration.TA_EPOCHS, exp_name = \"T_A\")\n",
        "results[\"T-A\"] = hist_ta[\"val_f1\"][-1]\n",
        "histories[\"T-A\"] = hist_ta\n",
        "\n",
        "# Running T-B: Unfreeze Layer4 + Head\n",
        "print(\"\\n===== Running T-B (Unfreeze Layer4 + Head) =====\")\n",
        "model_tb = build_resnet18_model(pretrained = True).to(configuration.DEVICE)\n",
        "freeze_except_layer4(model_tb)\n",
        "\n",
        "model_tb, hist_tb = train_model(model_tb, train_loader, val_loader, configuration.TB_EPOCHS, exp_name = \"T_B\")\n",
        "results[\"T-B\"] = hist_tb[\"val_f1\"][-1]\n",
        "histories[\"T-B\"] = hist_tb\n",
        "\n",
        "# Running T-C: Layer3 + Layer4 + Head)\n",
        "print(\"\\n===== Running T-C (Layer3 + Layer4 + Head) =====\")\n",
        "model_tc = build_resnet18_model(pretrained = True).to(configuration.DEVICE)\n",
        "tb_path = f\"{configuration.CHECKPOINT_DIR}/T_B_best.pt\"\n",
        "model_tc.load_state_dict(torch.load(tb_path, map_location = configuration.DEVICE))\n",
        "unfreeze_layer3_and_4(model_tc)\n",
        "\n",
        "model_tc, hist_tc = train_model(model_tc, train_loader, val_loader, configuration.TC_EPOCHS, exp_name = \"T_C\")\n",
        "results[\"T-C\"] = hist_tc[\"val_f1\"][-1]\n",
        "histories[\"T-C\"] = hist_tc\n",
        "\n",
        "# Running S-A: ResNet from Scratch\n",
        "print(\"\\n===== Running S-A (ResNet from Scratch) =====\")\n",
        "model_sa = build_resnet18_model(pretrained = False).to(configuration.DEVICE)\n",
        "model_sa, hist_sa = train_model(model_sa, train_loader, val_loader, configuration.SA_EPOCHS, exp_name = \"S_A\")\n",
        "results[\"S-A\"] = hist_sa[\"val_f1\"][-1]\n",
        "histories[\"S-A\"] = hist_sa\n",
        "\n",
        "# Select Best Model\n",
        "print(\"\\n Validation Results (Macro F1)\")\n",
        "for k, v in results.items():\n",
        "  print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "best_model_name = max(results, key = results.get)\n",
        "print (f\"\\n Best Model = {best_model_name}\")\n",
        "\n",
        "best_ckpt_name = {\n",
        "    \"T-A\": \"T_A_best.pt\",\n",
        "    \"T-B\": \"T_B_best.pt\",\n",
        "    \"T-C\": \"T_C_best.pt\",\n",
        "    \"S-A\": \"S_A_best.pt\"\n",
        "}[best_model_name]\n",
        "\n",
        "best_ckpt_path = f\"{configuration.CHECKPOINT_DIR}/{best_ckpt_name}\"\n",
        "print(f\"Best Model Checkpoint: {best_ckpt_path}\")\n",
        "\n",
        "\n",
        "for k, hist in histories.items():\n",
        "  plot_history(hist, title = k)\n",
        "\n",
        "\n",
        "# Loading best model for eval\n",
        "best_model = build_resnet18_model(pretrained = True).to(configuration.DEVICE)\n",
        "best_model.load_state_dict(torch.load(best_ckpt_path, map_location = configuration.DEVICE))\n",
        "best_model.eval()\n",
        "\n",
        "print(\"\\n Best model for test evaluation: \", best_model_name)\n",
        "\n",
        "\n",
        "# Flat Test Set eval\n",
        "class_to_idx = {c.lower(): i for i, c in enumerate(class_names)}\n",
        "class_to_idx.update({c: i for i, c in enumerate(class_names)})\n",
        "test_dataset = ASLTestSet(configuration.ASL_TEST_SET, eval_transform, class_to_idx)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "test_loss, test_metrics, y_true, y_pred = run_epoch(best_model, test_loader, criterion, optimizer = None)\n",
        "\n",
        "print(\"\\n Test Set Results:\")\n",
        "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
        "\n",
        "# Building Confusion Matrix\n",
        "def plot_conf_matrix(y_true, y_pred, labels):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  fig, ax = plt.subplots(figsize = (12, 12))\n",
        "  im = ax.imshow(cm, cmap = \"Blues\")\n",
        "\n",
        "  ax.set_xticks(np.arange(len(labels)))\n",
        "  ax.set_yticks(np.arange(len(labels)))\n",
        "  ax.set_xticklabels(labels, rotation = 90)\n",
        "  ax.set_yticklabels(labels)\n",
        "\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.colorbar(im)\n",
        "  plt.show()\n",
        "\n",
        "  plot_conf_matrix(y_true, y_pred, class_names)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "Ik0FR1xyECet",
        "outputId": "662f3f06-b79e-48f5-be88-0a04771bff12"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Building ASL Train/Val Split (Using 20% of dataset) ===\n",
            "Original dataset size: 87000\n",
            "Reduced dataset size:  17400\n",
            "Final Train size: 13920\n",
            "Final Val size:   3480\n",
            "Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
            "\n",
            "===== Running T-A (Head Only) =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2591303232.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mfreeze_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_ta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTA_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"T_A\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"T-A\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist_ta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_f1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mhistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"T-A\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist_ta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-159200523.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, exp_name)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-159200523.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m    159\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3522\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3524\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3526\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 2: Dynamic WLASL100 Word Recognition LSTM\n",
        "\n",
        "\n",
        "\n",
        "def load_video_frames(path, num_frames = configuration.FRAME_NUM):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames = []\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    frames.append(frame[:, :, ::-1])\n",
        "  cap.release()\n",
        "\n",
        "  if len(frames) == 0:\n",
        "    frames = [np.zeros((224, 224, 3), dtype = np.uint8)]* num_frames\n",
        "\n",
        "  idxs = np.linspace(0, len(frames) - 1, num_frames).astype(int)\n",
        "  sampled = [frames[i] for i in idxs]\n",
        "\n",
        "  sampled = [Image.fromarray(f) for f in sampled]\n",
        "  return sampled\n",
        "\n",
        "\n",
        "# WLASL Dataset Class\n",
        "class WLASL_Dataset(Dataset):\n",
        "    def __init__(self, root, split, transform=eval_transform, num_frames=16, reduction_ratio = 1.0):\n",
        "        self.transform = transform\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "        split_dir = os.path.join(root, split)\n",
        "        self.samples = []\n",
        "\n",
        "        self.class_names = sorted(os.listdir(split_dir))\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.class_names)}\n",
        "\n",
        "        for cls in self.class_names:\n",
        "            cls_dir = os.path.join(split_dir, cls)\n",
        "            videos = [f for f in os.listdir(cls_dir) if f.endswith(\".mp4\")]\n",
        "\n",
        "            # Reduce videos per class\n",
        "            keep = int(len(videos) * reduction_ratio)\n",
        "            videos = videos[:max(1, keep)]\n",
        "\n",
        "            for fname in videos:\n",
        "                path = os.path.join(cls_dir, fname)\n",
        "                self.samples.append((path, self.class_to_idx[cls]))\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} {split} videos (Reduction ratio = {reduction_ratio})\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        frames = load_video_frames(path, num_frames = self.num_frames)\n",
        "        frames = [self.transform(f) for f in frames]\n",
        "\n",
        "        return torch.stack(frames, dim = 0), label\n",
        "\n",
        "# Import Best Phase 1 Model\n",
        "def load_best_model(best_ckpt_path):\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    model.fc = nn.Identity()\n",
        "\n",
        "    # Load checkpoint FIRST\n",
        "    checkpoint = torch.load(best_ckpt_path, map_location=configuration.DEVICE)\n",
        "\n",
        "    # Then load state dict with strict=False\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "    model.to(configuration.DEVICE)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "# Temp Model LSTM\n",
        "class TempLSTM(nn.Module):\n",
        "  def __init__(self, feature_dim = 512, hidden_dim = 256, num_classes = 100):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size = feature_dim,\n",
        "        hidden_size = hidden_dim,\n",
        "        batch_first = True\n",
        "    )\n",
        "    self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, _ = self.lstm(x)\n",
        "    last_hidden = out[:, -1, :]\n",
        "    logits = self.fc(last_hidden)\n",
        "    return logits\n",
        "\n",
        "# Feature Extraction Function\n",
        "def extract_features(model, frames):\n",
        "  B, T, C, H, W = frames.shape\n",
        "  frames = frames.view(B*T, C, H, W)\n",
        "  with torch.no_grad():\n",
        "    feats = model(frames.to(configuration.DEVICE))\n",
        "  return feats.view(B, T, -1)\n",
        "\n",
        "\n",
        "# Training Loop for Phase 2\n",
        "def train_phase2(feature_model, train_loader, val_loader, trainable_layer4 = False):\n",
        "  if trainable_layer4:\n",
        "    print(\" Unfreezing layer4 of resnet\")\n",
        "    for name, p in feature_model.named_parameters():\n",
        "      if name.startswith(\"layer4\"):\n",
        "        p.required_grad = True\n",
        "      else:\n",
        "        p.required_grad = False\n",
        "  else:\n",
        "    print(\"Freezing\")\n",
        "    for p in feature_model.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "  lstm_head = TempLSTM(num_classes = len(train_loader.dataset.class_names)).to(configuration.DEVICE)\n",
        "  params = list(filter(lambda p: p.requires_grad, feature_model.parameters())) + list(lstm_head.parameters())\n",
        "\n",
        "  optimizer = torch.optim.Adam(params, lr = configuration.P2_LR)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimal_f1 = -1\n",
        "  best_state = None\n",
        "\n",
        "  for epoch in range(configuration.P2_EPOCHS):\n",
        "    feature_model.train()\n",
        "    lstm_head.train()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    total_loss = 0\n",
        "\n",
        "    for frames, labels in train_loader:\n",
        "      frames = frames.to(configuration.DEVICE)\n",
        "      labels = labels.to(configuration.DEVICE)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      feats = extract_features(feature_model, frames)\n",
        "      logits = lstm_head(feats)\n",
        "      loss = criterion(logits, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item() * frames.size(0)\n",
        "\n",
        "      preds = logits.argmax(dim = 1).detach().cpu().numpy()\n",
        "      y_true.extend(labels.cpu().numpy())\n",
        "      y_pred.extend(preds)\n",
        "\n",
        "    train_f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
        "    train_loss = total_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{configuration.P2_EPOCHS} | Loss={train_loss:.4f} | F1={train_f1:.4f}\")\n",
        "\n",
        "\n",
        "    feature_model.eval()\n",
        "    lstm_head.eval()\n",
        "    val_true = []\n",
        "    val_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for frames, labels in val_loader:\n",
        "        feats = extract_features(feature_model, frames.to(configuration.DEVICE))\n",
        "        logits = lstm_head(feats)\n",
        "        preds = logits.argmax(dim = 1).cpu().numpy()\n",
        "        val_pred.extend(preds)\n",
        "        val_true.extend(labels.numpy())\n",
        "\n",
        "    val_f1 = f1_score(val_true, val_pred, average = \"macro\")\n",
        "    print(f\"Val F1={val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > optimal_f1:\n",
        "      optimal_f1 = val_f1\n",
        "      best_state = {\n",
        "          \"cnn\": feature_model.state_dict(),\n",
        "          \"head\": lstm_head.state_dict()\n",
        "      }\n",
        "\n",
        "  return best_state, optimal_f1\n",
        "\n",
        "\n",
        "# Load WLASL Data\n",
        "train_wlasl = WLASL_Dataset(configuration.WLASL100_SET, \"train\", reduction_ratio = 0.005)\n",
        "val_wlasl = WLASL_Dataset(configuration.WLASL100_SET, \"val\", reduction_ratio = 0.005)\n",
        "test_wlasl = WLASL_Dataset(configuration.WLASL100_SET, \"test\", reduction_ratio = 0.05)\n",
        "\n",
        "train_loader_w = DataLoader(train_wlasl, batch_size = 4, shuffle = True)\n",
        "val_loader_w = DataLoader(val_wlasl, batch_size = 4, shuffle = False)\n",
        "test_loader_w = DataLoader(test_wlasl, batch_size = 4, shuffle = False)\n",
        "\n",
        "# Load Best Phase 1 Model\n",
        "print(\"\\n Loading Phase 1 Best Model for Feature Extraction\")\n",
        "feature_model = load_best_model(best_ckpt_path)\n",
        "\n",
        "# Phase 2: Frozen CNN\n",
        "print(\"\\n===== Running Phase 2 (Frozen CNN) =====\")\n",
        "phase2A_state, phase2A_val_f1 = train_phase2(feature_model, train_loader_w, val_loader_w, trainable_layer4 = False)\n",
        "print(f\"Phase 2.0 Best Val F1={phase2A_val_f1:.4f}\")\n",
        "\n",
        "# Phase 2.5: Unfreeze Layer 4\n",
        "print(\"\\n===== Running Phase 2.5 (Unfreeze Layer 4) =====\")\n",
        "feature_model = load_best_model(best_ckpt_path)\n",
        "phase2B_state, phase2B_val_f1 = train_phase2(feature_model, train_loader_w, val_loader_w, trainable_layer4 = True)\n",
        "print(f\"Phase 2.5 Best Val F1={phase2B_val_f1:.4f}\")\n",
        "\n",
        "# Evaluate Models\n",
        "best_phase = \"2.0\" if phase2A_val_f1 > phase2B_val_f1 else \"2.5\"\n",
        "print(f\"Best Model = PHASE {best_phase}\")\n",
        "\n",
        "if best_phase == \"2.0\":\n",
        "  cnn_state = phase2A_state[\"cnn\"]\n",
        "  head_state = phase2A_state[\"head\"]\n",
        "else:\n",
        "  cnn_state = phase2B_state[\"cnn\"]\n",
        "  head_state = phase2B_state[\"head\"]\n",
        "\n",
        "feature_model = load_best_model(best_ckpt_path)\n",
        "feature_model.load_state_dict(cnn_state)\n",
        "lstm_head = TempLSTM(num_classes = len(train_loader_w.dataset.class_names)).to(configuration.DEVICE)\n",
        "lstm_head.load_state_dict(head_state)\n",
        "\n",
        "# Test Set Eval for P2\n",
        "print(\"\\n Phase 2 Test Eval\")\n",
        "feature_model.eval()\n",
        "lstm_head.eval()\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for frames, labels in test_loader_w:\n",
        "    feats = extract_features(feature_model, frames.to(configuration.DEVICE))\n",
        "    logits = lstm_head(feats)\n",
        "    preds = logits.argmax(dim = 1).cpu().numpy()\n",
        "    y_pred.extend(preds)\n",
        "    y_true.extend(labels.numpy())\n",
        "\n",
        "phase2_test_acc = accuracy_score(y_true, y_pred)\n",
        "phase2_test_f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
        "\n",
        "print(f\"Testing Accuracy: {phase2_test_acc:.4f}\")\n",
        "print(f\"Testing F1: {phase2_test_f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "z1ZkeWU3H82z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "662fa735-e5c7-4d3a-a6a5-fe096e0fd7da"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 train videos (Reduction ratio = 0.05)\n",
            "Loaded 100 val videos (Reduction ratio = 0.05)\n",
            "Loaded 97 test videos (Reduction ratio = 0.05)\n",
            "\n",
            " Loading Phase 1 Best Model for Feature Extraction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Running Phase 2 (Frozen CNN) =====\n",
            "Freezing\n",
            "Epoch 1/5 | Loss=4.6650 | F1=0.0005\n",
            "Val F1=0.0000\n",
            "Epoch 2/5 | Loss=4.5721 | F1=0.0055\n",
            "Val F1=0.0000\n",
            "Epoch 3/5 | Loss=4.5176 | F1=0.0164\n",
            "Val F1=0.0000\n",
            "Epoch 4/5 | Loss=4.4575 | F1=0.0552\n",
            "Val F1=0.0000\n",
            "Epoch 5/5 | Loss=4.3778 | F1=0.0648\n",
            "Val F1=0.0000\n",
            "Phase 2.0 Best Val F1=0.0000\n",
            "\n",
            "===== Running Phase 2.5 (Unfreeze Layer 4) =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Unfreezing layer4 of resnet\n",
            "Epoch 1/5 | Loss=4.6712 | F1=0.0003\n",
            "Val F1=0.0003\n",
            "Epoch 2/5 | Loss=4.5701 | F1=0.0003\n",
            "Val F1=0.0004\n",
            "Epoch 3/5 | Loss=4.5135 | F1=0.0033\n",
            "Val F1=0.0000\n",
            "Epoch 4/5 | Loss=4.4513 | F1=0.0567\n",
            "Val F1=0.0079\n",
            "Epoch 5/5 | Loss=4.3927 | F1=0.0626\n",
            "Val F1=0.0100\n",
            "Phase 2.5 Best Val F1=0.0100\n",
            "Best Model = PHASE 2.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Phase 2 Test Eval\n",
            "Testing Accuracy: 0.0103\n",
            "Testing F1: 0.0051\n"
          ]
        }
      ]
    }
  ]
}